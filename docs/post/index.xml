<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on João Blasques | AI-Enabled Data Engineer</title><link>https://joaoblasques.com/post/</link><description>Recent content in Posts on João Blasques | AI-Enabled Data Engineer</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 14 Jul 2025 00:00:00 +0100</lastBuildDate><atom:link href="https://joaoblasques.com/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Analytics Engineering with dbt: From Raw Data to Business Intelligence</title><link>https://joaoblasques.com/post/data-pipeline-transformation-analytics/</link><pubDate>Mon, 14 Jul 2025 00:00:00 +0100</pubDate><guid>https://joaoblasques.com/post/data-pipeline-transformation-analytics/</guid><description>&lt;h2 id="project-overview">Project Overview&lt;/h2>
&lt;p>This project demonstrates the implementation of a comprehensive analytics engineering pipeline using dbt (data build tool) as the primary transformation layer. The pipeline showcases modern data engineering practices including ELT methodology, dimensional modeling, automated testing, and business intelligence visualization.&lt;/p>
&lt;p>&lt;strong>Repository:&lt;/strong> &lt;a href="https://github.com/joaoblasques/data-pipeline-transformation-analytics">Analytics Engineering with dbt&lt;/a>&lt;/p>
&lt;p>The project focuses on transforming raw NYC taxi trip data into business-ready analytics tables using dbt&amp;rsquo;s modular approach, implementing both dbt Cloud and dbt Core workflows, and creating interactive dashboards with Looker Studio.&lt;/p>
&lt;h2 id="key-concepts">Key Concepts&lt;/h2>
&lt;p>• &lt;strong>Analytics Engineering:&lt;/strong> Bridging the gap between data engineering and data analysis with software engineering best practices
• &lt;strong>ELT vs ETL:&lt;/strong> Leveraging cloud data warehouses for in-database transformations
• &lt;strong>Dimensional Modeling:&lt;/strong> Implementing Kimball&amp;rsquo;s star schema methodology for analytical workloads
• &lt;strong>dbt Fundamentals:&lt;/strong> Models, macros, packages, variables, and testing frameworks
• &lt;strong>Data Governance:&lt;/strong> Testing, documentation, and deployment strategies
• &lt;strong>Business Intelligence:&lt;/strong> Creating interactive dashboards and visualizations&lt;/p></description></item><item><title>Building a Data Pipeline with BigQuery: From Storage to Analytics</title><link>https://joaoblasques.com/post/data-warehouse-bigquery-pipeline/</link><pubDate>Mon, 14 Jul 2025 00:00:00 +0100</pubDate><guid>https://joaoblasques.com/post/data-warehouse-bigquery-pipeline/</guid><description>&lt;h2 id="project-overview">Project Overview&lt;/h2>
&lt;p>This project demonstrates the implementation of a comprehensive data pipeline using Google BigQuery as the primary data warehouse solution. The pipeline showcases modern data engineering practices including external data integration, table optimization strategies, and performance tuning techniques.&lt;/p>
&lt;p>&lt;strong>Repository:&lt;/strong> &lt;a href="https://github.com/jonasblasques/4-data-pipeline-datawarehouse-bigquery">Data Pipeline with BigQuery&lt;/a>&lt;/p>
&lt;p>The project focuses on building a scalable, cost-effective data warehouse solution that can handle large volumes of NYC taxi trip data while maintaining optimal query performance and cost efficiency.&lt;/p>
&lt;h2 id="key-concepts">Key Concepts&lt;/h2>
&lt;p>• &lt;strong>OLAP vs OLTP:&lt;/strong> Understanding the fundamental differences between Online Analytical Processing and Online Transaction Processing systems
• &lt;strong>Data Warehousing:&lt;/strong> Implementing centralized storage for analytical workloads with optimized query performance
• &lt;strong>Table Partitioning:&lt;/strong> Dividing large tables into manageable chunks based on time or range values
• &lt;strong>Clustering:&lt;/strong> Organizing data within partitions to improve query performance and reduce costs
• &lt;strong>External Tables:&lt;/strong> Querying data stored outside BigQuery without incurring storage costs
• &lt;strong>Performance Optimization:&lt;/strong> Implementing best practices for cost reduction and query efficiency&lt;/p></description></item><item><title>Data Pipeline Orchestration using Kestra</title><link>https://joaoblasques.com/post/data-pipeline-orchestration-kestra/</link><pubDate>Sat, 21 Jun 2025 09:30:00 +0100</pubDate><guid>https://joaoblasques.com/post/data-pipeline-orchestration-kestra/</guid><description>&lt;h2 id="project-overview">Project Overview&lt;/h2>
&lt;p>This &lt;a href="https://github.com/joaoblasques/data-pipeline-orchestration-kestra">repository&lt;/a> demonstrates workflow orchestration for data engineering pipelines using &lt;a href="https://kestra.io/">Kestra&lt;/a>. It guides users through building, running, and scheduling data pipelines that extract, transform, and load (ETL) data both locally (with PostgreSQL) and in the cloud (with Google Cloud Platform). The project is hands-on and includes conceptual explanations, infrastructure setup, and several example pipeline flows.&lt;/p>
&lt;hr>
&lt;h2 id="key-concepts">Key Concepts&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Workflow Orchestration:&lt;/strong> Automating and managing complex workflows with dependencies, retries, logging, and monitoring.&lt;/li>
&lt;li>&lt;strong>Kestra:&lt;/strong> An orchestration platform with a user-friendly UI and YAML-based workflow definitions (called &amp;ldquo;flows&amp;rdquo;).&lt;/li>
&lt;li>&lt;strong>Data Lake &amp;amp; Data Warehouse:&lt;/strong> Demonstrates moving data from raw storage (GCS) to structured analytics (BigQuery).&lt;/li>
&lt;/ul></description></item><item><title>Orchestrating Data Pipelines with Apache Airflow: A Comprehensive Guide</title><link>https://joaoblasques.com/post/data-pipeline-orchestration-airflow/</link><pubDate>Sat, 21 Jun 2025 09:30:00 +0100</pubDate><guid>https://joaoblasques.com/post/data-pipeline-orchestration-airflow/</guid><description>&lt;h2 id="project-overview">Project Overview&lt;/h2>
&lt;p>This &lt;a href="https://github.com/joaoblasques/data-pipeline-orchestration-airflow">repository&lt;/a> serves as a practical guide to building and orchestrating robust data pipelines using Apache Airflow. It covers essential concepts from basic workflow management to advanced deployments with Google Cloud Platform (GCP) and Kubernetes.&lt;/p>
&lt;hr>
&lt;h2 id="key-concepts">Key Concepts&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Workflow Orchestration:&lt;/strong> Automating and managing complex data workflows with dependencies, scheduling, retries, and monitoring using Apache Airflow.&lt;/li>
&lt;li>&lt;strong>DAGs (Directed Acyclic Graphs):&lt;/strong> The core abstraction in Airflow for defining task dependencies, execution order, and workflow logic.&lt;/li>
&lt;li>&lt;strong>Extensible Operators &amp;amp; Integrations:&lt;/strong> Leveraging Airflow&amp;rsquo;s wide range of built-in operators and custom plugins to interact with databases, cloud services (GCP, Kubernetes), and external systems.&lt;/li>
&lt;li>&lt;strong>Scalable Deployments:&lt;/strong> Running Airflow locally for prototyping, or deploying on cloud and Kubernetes for production-scale, resilient, and distributed data pipeline execution.&lt;/li>
&lt;/ul></description></item><item><title>Simple Data Pipeline</title><link>https://joaoblasques.com/post/data-pipeline-simple/</link><pubDate>Sat, 21 Jun 2025 09:30:00 +0100</pubDate><guid>https://joaoblasques.com/post/data-pipeline-simple/</guid><description>&lt;h2 id="project-overview">Project Overview&lt;/h2>
&lt;p>This &lt;a href="https://github.com/joaoblasques/data-pipeline-simple">repository&lt;/a> provides a comprehensive, step-by-step guide to building a simple data engineering pipeline using containerization (Docker), orchestration (Docker Compose), and Infrastructure as Code (Terraform), with a focus on ingesting and processing NYC taxi data. The project is hands-on and includes conceptual explanations, infrastructure setup, and several example pipeline flows.&lt;/p>
&lt;p>This project is a practical template for data engineers to learn and implement containerized data pipelines, local and cloud database management, and automated cloud infrastructure provisioning using modern tools like Docker, Docker Compose, and Terraform. It is especially useful for those looking to understand the end-to-end workflow from local prototyping to cloud deployment in a reproducible, automated way.&lt;/p></description></item><item><title>The Role of AI in Modern Data Architectures</title><link>https://joaoblasques.com/post/the-role-of-ai-in-modern-data-architectures/</link><pubDate>Sun, 11 May 2025 12:15:00 +0100</pubDate><guid>https://joaoblasques.com/post/the-role-of-ai-in-modern-data-architectures/</guid><description>&lt;h2 id="ai-driven-data-architecture">AI-Driven Data Architecture&lt;/h2>
&lt;p>Artificial intelligence isn&amp;rsquo;t just a consumer of data—it&amp;rsquo;s increasingly becoming an integral part of how we design and operate our data systems. This post explores the evolving relationship between AI and data architecture.&lt;/p>
&lt;h3 id="ai-enhanced-data-processing">AI-Enhanced Data Processing&lt;/h3>
&lt;p>Modern data architectures are incorporating AI at various levels:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Intelligent Data Cataloging&lt;/strong> - Automatically discovering, classifying, and tagging data assets&lt;/li>
&lt;li>&lt;strong>Adaptive Data Integration&lt;/strong> - Using ML to identify optimal integration patterns and transformations&lt;/li>
&lt;li>&lt;strong>Automated Quality Management&lt;/strong> - Detecting anomalies and quality issues without manual rules&lt;/li>
&lt;li>&lt;strong>Self-Tuning Systems&lt;/strong> - Databases and data platforms that optimize themselves based on workloads&lt;/li>
&lt;/ul>
&lt;h3 id="real-world-applications">Real-World Applications&lt;/h3>
&lt;h4 id="recommendation-systems">Recommendation Systems&lt;/h4>
&lt;p>AI algorithms help determine which data is most relevant to different users and use cases, optimizing data discovery and access.&lt;/p></description></item><item><title>Machine Learning Pipeline Design</title><link>https://joaoblasques.com/post/ml-pipeline-design/</link><pubDate>Fri, 09 May 2025 10:45:00 +0100</pubDate><guid>https://joaoblasques.com/post/ml-pipeline-design/</guid><description>&lt;h2 id="building-effective-machine-learning-pipelines">Building Effective Machine Learning Pipelines&lt;/h2>
&lt;p>Creating robust machine learning pipelines is essential for deploying AI solutions at scale. This post covers key considerations and best practices.&lt;/p>
&lt;h3 id="the-anatomy-of-an-ml-pipeline">The Anatomy of an ML Pipeline&lt;/h3>
&lt;p>A well-designed ML pipeline includes these key stages:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Data Ingestion&lt;/strong> - Collecting and importing data from various sources&lt;/li>
&lt;li>&lt;strong>Data Preparation&lt;/strong> - Cleaning, transforming, and feature engineering&lt;/li>
&lt;li>&lt;strong>Model Training&lt;/strong> - Developing and tuning ML models&lt;/li>
&lt;li>&lt;strong>Model Evaluation&lt;/strong> - Assessing performance and validity&lt;/li>
&lt;li>&lt;strong>Model Deployment&lt;/strong> - Serving models in production environments&lt;/li>
&lt;li>&lt;strong>Monitoring&lt;/strong> - Tracking performance and detecting drift&lt;/li>
&lt;/ol>
&lt;h3 id="common-challenges-and-solutions">Common Challenges and Solutions&lt;/h3>
&lt;h4 id="challenge-data-quality-issues">Challenge: Data Quality Issues&lt;/h4>
&lt;p>&lt;strong>Solution&lt;/strong>: Implement robust data validation and cleaning processes early in the pipeline.&lt;/p></description></item><item><title>Getting Started with Data Engineering</title><link>https://joaoblasques.com/post/getting-started-with-data-engineering/</link><pubDate>Mon, 05 May 2025 09:30:00 +0100</pubDate><guid>https://joaoblasques.com/post/getting-started-with-data-engineering/</guid><description>&lt;h2 id="data-engineering-fundamentals">Data Engineering Fundamentals&lt;/h2>
&lt;p>Data engineering is the backbone of any data-driven organization. In this post, we will explore the fundamental concepts that every aspiring data engineer should understand.&lt;/p>
&lt;h3 id="what-is-data-engineering">What is Data Engineering?&lt;/h3>
&lt;p>Data engineering focuses on designing, building, and maintaining the infrastructure and architecture for data generation, storage, and analysis. Data engineers develop the systems that collect, manage, and convert raw data into usable information for data scientists and business analysts.&lt;/p></description></item><item><title>Welcome to My Professional Website</title><link>https://joaoblasques.com/post/welcome/</link><pubDate>Wed, 23 Apr 2025 15:55:33 +0100</pubDate><guid>https://joaoblasques.com/post/welcome/</guid><description>&lt;h2 id="hello-im-joão-blasques">Hello, I&amp;rsquo;m João Blasques&lt;/h2>
&lt;p>Welcome to my professional website! I&amp;rsquo;m an AI-Enabled Data Engineer passionate about leveraging artificial intelligence and data solutions to solve complex business problems.&lt;/p>
&lt;h3 id="my-background">My Background&lt;/h3>
&lt;p>With expertise in data engineering, machine learning, and AI integration, I help organizations transform their data into actionable insights. I specialize in designing and implementing data pipelines, creating machine learning models, and developing AI-powered applications that drive business value.&lt;/p>
&lt;h3 id="what-youll-find-here">What You&amp;rsquo;ll Find Here&lt;/h3>
&lt;p>On this website, you can explore:&lt;/p></description></item></channel></rss>